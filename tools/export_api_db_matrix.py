from __future__ import annotations

import json
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any
import uuid

import pandas as pd
from openpyxl.utils import get_column_letter
from openpyxl.worksheet.table import Table, TableStyleInfo


# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’åŸºæº–ã«ã—ãŸãƒ‘ã‚¹
PROJECT_ROOT = Path(__file__).resolve().parents[1]
SCHEMA_SQL = PROJECT_ROOT / "docs" / "schema" / "03_db_schema_current.sql"
OPENAPI_JSON = PROJECT_ROOT / "docs" / "schema" / "current_openapi.json"
OUTPUT_XLSX = PROJECT_ROOT / "docs" / "schema" / "api_db_matrix.xlsx"


@dataclass
class ColumnDef:
    table_name: str
    column_name: str
    data_type: str
    is_primary_key: bool = False
    foreign_key: str | None = None
    raw_line: str = ""


@dataclass
class EndpointDef:
    path: str
    method: str
    summary: str
    tags: str
    operation_id: str


def parse_schema_sql(path: Path) -> list[ColumnDef]:
    """
    docs/schema/03_db_schema_current.sql ã‹ã‚‰
    ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ å®šç¾© + PK/FK ã‚’æŠœãå‡ºã™ã€‚
    """
    text = path.read_text(encoding="utf-8")

    # CREATE TABLE ... (...) ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å˜ä½ã§æŠœãå‡ºã™
    table_blocks = re.split(r"(?i)\bCREATE\s+TABLE\b", text)

    results: list[ColumnDef] = []

    for block in table_blocks[1:]:
        header_and_body = block.split("(", 1)
        if len(header_and_body) != 2:
            continue

        header, body = header_and_body
        # public.orders ã‚„ orders ã‚’æ‹¾ã†
        m_table = re.search(r"(?P<name>(?:\w+\.)?\w+)", header.strip())
        if not m_table:
            continue

        full_table_name = m_table.group("name")
        table_name = full_table_name.split(".")[-1]

        body_before_close = re.split(r"\);\s*", body, 1)[0]
        lines = [ln.rstrip() for ln in body_before_close.splitlines()]

        # 1ãƒ‘ã‚¹ç›®: ã‚«ãƒ©ãƒ å®šç¾©
        columns_by_name: dict[str, ColumnDef] = {}
        column_order: list[str] = []

        for line in lines:
            line_stripped = line.strip()
            if not line_stripped:
                continue

            upper = line_stripped.upper()
            if upper.startswith(("CONSTRAINT", "PRIMARY KEY", "UNIQUE", "FOREIGN KEY", "CHECK")):
                # åˆ¶ç´„è¡Œã¯åˆ¥ã§å‡¦ç†
                continue

            # ä¾‹: id bigint GENERATED BY DEFAULT AS IDENTITY,
            m_col = re.match(
                r'^"?(?P<col>\w+)"?\s+(?P<type>[^,]+?)(?:,)?$',
                line_stripped,
            )
            if not m_col:
                continue

            col_name = m_col.group("col")
            data_type = m_col.group("type").strip()

            col_def = ColumnDef(
                table_name=table_name,
                column_name=col_name,
                data_type=data_type,
                raw_line=line_stripped,
            )
            columns_by_name[col_name] = col_def
            column_order.append(col_name)

        # 2ãƒ‘ã‚¹ç›®: åˆ¶ç´„è¡Œã‹ã‚‰ PK/FK ã‚’æŠ½å‡º
        for line in lines:
            line_stripped = line.strip()
            if not line_stripped:
                continue

            upper = line_stripped.upper()
            if "PRIMARY KEY" in upper:
                m_pk = re.search(r"PRIMARY\s+KEY\s*\((?P<cols>[^)]+)\)", line_stripped, re.IGNORECASE)
                if m_pk:
                    cols = [c.strip().strip('"') for c in m_pk.group("cols").split(",")]
                    for col in cols:
                        if col in columns_by_name:
                            columns_by_name[col].is_primary_key = True

            if "FOREIGN KEY" in upper and "REFERENCES" in upper:
                m_fk = re.search(
                    r"FOREIGN\s+KEY\s*\((?P<cols>[^)]+)\)\s*REFERENCES\s+(?P<reftable>[\w\.]+)\s*\((?P<refcols>[^)]+)\)",
                    line_stripped,
                    re.IGNORECASE,
                )
                if m_fk:
                    local_cols = [c.strip().strip('"') for c in m_fk.group("cols").split(",")]
                    ref_table = m_fk.group("reftable").split(".")[-1]
                    ref_cols = [c.strip().strip('"') for c in m_fk.group("refcols").split(",")]

                    # å˜ä¸€ or åŒæ•°åˆ—å¯¾å¿œï¼ˆãã‚Œä»¥ä¸Šã¯é›‘ã«1:1å¯¾å¿œï¼‰
                    for idx, local_col in enumerate(local_cols):
                        if local_col not in columns_by_name:
                            continue
                        ref_col = ref_cols[min(idx, len(ref_cols) - 1)]
                        columns_by_name[local_col].foreign_key = f"{ref_table}.{ref_col}"

        # ã‚«ãƒ©ãƒ é †ã§ results ã«è¿½åŠ 
        for col_name in column_order:
            results.append(columns_by_name[col_name])

    return results


def parse_openapi(path: Path) -> list[EndpointDef]:
    """
    OpenAPI JSON ã‹ã‚‰ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä¸€è¦§ã‚’å–ã‚Šå‡ºã™ã€‚
    """
    spec: dict[str, Any] = json.loads(path.read_text(encoding="utf-8"))

    endpoints: list[EndpointDef] = []

    for path_str, methods in spec.get("paths", {}).items():
        if not isinstance(methods, dict):
            continue
        for method, op in methods.items():
            if not isinstance(op, dict):
                continue

            summary = op.get("summary", "") or ""
            tags_list = op.get("tags", []) or []
            tags = ", ".join(tags_list)
            operation_id = op.get("operationId", "") or ""

            endpoints.append(
                EndpointDef(
                    path=path_str,
                    method=method.upper(),
                    summary=summary,
                    tags=tags,
                    operation_id=operation_id,
                )
            )

    return endpoints


def build_table_api_matrix(
    columns: list[ColumnDef], endpoints: list[EndpointDef]
) -> pd.DataFrame:
    """
    ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã«ã€ŒAPI ã‚ã‚Š/ãªã—ã€ã¨ã€
    é–¢é€£ã—ãã†ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä¸€è¦§ã‚’ä½œæˆã€‚
    """
    table_names = sorted({c.table_name for c in columns})
    rows: list[dict[str, Any]] = []

    for table in table_names:
        keyword = f"/{table}"
        related = [
            f"{ep.method} {ep.path}"
            for ep in endpoints
            if keyword in ep.path
        ]
        rows.append(
            {
                "table_name": table,
                "has_api": bool(related),
                "related_endpoints": "\n".join(sorted(set(related))),
            }
        )

    df = pd.DataFrame(rows)
    # True/False -> â—‹/Ã— ã«å¤‰æ›
    df["has_api"] = df["has_api"].map(lambda v: "â—‹" if v else "Ã—")
    return df


def make_sheet_name(table_name: str) -> str:
    """Excel ã‚·ãƒ¼ãƒˆåç”¨ã«å®‰å…¨ãªåå‰ã‚’ä½œã‚‹ï¼ˆ31æ–‡å­—åˆ¶é™å¯¾å¿œï¼‰ã€‚"""
    base = f"tbl_{table_name}"
    # / \ ? * [ ] ãªã©ã‚’é¿ã‘ã‚‹
    safe = re.sub(r"[][*/?:\\]", "_", base)
    return safe[:31]


def add_excel_table(ws, style_name: str) -> None:
    """ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’ Excel ã®ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã—ã€ãƒ†ãƒ¼ãƒ–ãƒ«åã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã§ä¸€æ„ã«ã™ã‚‹"""
    if ws.max_row < 1 or ws.max_column < 1:
        return

    ref = f"A1:{get_column_letter(ws.max_column)}{ws.max_row}"

    # åŸºæœ¬ã®ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆã‚·ãƒ¼ãƒˆåãƒ™ãƒ¼ã‚¹ï¼‰
    base_name = "tbl_" + re.sub(r"[^0-9A-Za-z_]", "_", ws.title)
    base_name = base_name[:20]  # å°‘ã—çŸ­ã‚ã«ã—ã¦ãŠã

    # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä»˜ã‘ã¦ã»ã¼ç¢ºå®Ÿã«ä¸€æ„ã«ã™ã‚‹
    suffix = uuid.uuid4().hex[:6]
    table_name = f"{base_name}_{suffix}"

    table = Table(displayName=table_name, ref=ref)

    style = TableStyleInfo(
        name=style_name,
        showFirstColumn=False,
        showLastColumn=False,
        showRowStripes=True,
        showColumnStripes=False,
    )
    table.tableStyleInfo = style
    ws.add_table(table)




def main() -> None:
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    if OUTPUT_XLSX.exists():
        try:
            OUTPUT_XLSX.unlink()
            print(f"ğŸ—‘ï¸ Removed existing file: {OUTPUT_XLSX}")
        except Exception:
            raise SystemExit(
                f"âŒ Cannot delete existing file: {OUTPUT_XLSX}\n"
                f"Excel ã§é–‹ãã£ã±ãªã—ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚é–‰ã˜ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )
    if not SCHEMA_SQL.exists():
        raise SystemExit(f"Schema SQL not found: {SCHEMA_SQL}")

    if not OPENAPI_JSON.exists():
        raise SystemExit(
            f"OpenAPI JSON not found: {OPENAPI_JSON}\n"
            "å…ˆã« backend ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§:\n"
            "  python openapi_diff_check.py generate ../docs/schema/current_openapi.json\n"
            "ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    print(f"ğŸ“„ Loading schema SQL: {SCHEMA_SQL}")
    columns = parse_schema_sql(SCHEMA_SQL)
    print(f"  -> {len({c.table_name for c in columns})} tables, {len(columns)} columns")

    print(f"ğŸ“„ Loading OpenAPI JSON: {OPENAPI_JSON}")
    endpoints = parse_openapi(OPENAPI_JSON)
    print(f"  -> {len(endpoints)} endpoints")

    # ----- DataFrame ç”Ÿæˆ -----
    df_tables = pd.DataFrame(
        [
            {
                "table_name": c.table_name,
                "column_name": c.column_name,
                "data_type": c.data_type,
                "is_primary_key": c.is_primary_key,
                "foreign_key": c.foreign_key or "",
                "raw_line": c.raw_line,
            }
            for c in columns
        ]
    ).sort_values(["table_name", "column_name"])

    df_endpoints = pd.DataFrame(
        [
            {
                "path": ep.path,
                "method": ep.method,
                "summary": ep.summary,
                "tags": ep.tags,
                "operation_id": ep.operation_id,
            }
            for ep in endpoints
        ]
    ).sort_values(["path", "method"])

    df_matrix = build_table_api_matrix(columns, endpoints)

    # ----- Excel å‡ºåŠ› -----
    OUTPUT_XLSX.parent.mkdir(parents=True, exist_ok=True)
    with pd.ExcelWriter(OUTPUT_XLSX, engine="openpyxl") as writer:
        # å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚·ãƒ¼ãƒˆ
        df_tables.to_excel(writer, sheet_name="tables_all", index=False)
        df_endpoints.to_excel(writer, sheet_name="endpoints", index=False)
        df_matrix.to_excel(writer, sheet_name="table_api_matrix", index=False)

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®ã‚·ãƒ¼ãƒˆ
        for table_name, group in df_tables.groupby("table_name"):
            sheet_name = make_sheet_name(table_name)
            # äººé–“ãŒè¦‹ã‚‹æƒ³å®šã®åˆ—æ§‹æˆ
            df_table_sheet = pd.DataFrame(
                {
                    "column_name": group["column_name"].values,
                    "data_type": group["data_type"].values,
                    # PK ã¯ â—‹ / ç©ºæ¬„ ã§è¡¨ç¾
                    "PK": ["â—‹" if v else "" for v in group["is_primary_key"].values],
                    "FK": group["foreign_key"].values,
                    "japanese_label": ["" for _ in range(len(group))],
                    "business_note": ["" for _ in range(len(group))],
                    # åˆæœŸå€¤ã¯ column_name = Python ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åæƒ³å®š
                    "python_field": group["column_name"].values,
                }
            )

            df_table_sheet.to_excel(writer, sheet_name=sheet_name, index=False)

        # ---- ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨ï¼ˆExcel ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ï¼‰----
        workbook = writer.book

        for ws in workbook.worksheets:
            if ws.title.startswith("tbl_"):
                # ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥ã‚·ãƒ¼ãƒˆ: è–„ã„ãƒ©ã‚¤ãƒˆç³»
                add_excel_table(ws, style_name="TableStyleLight9")
            elif ws.title == "endpoints":
                # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä¸€è¦§: å°‘ã—æ¿ƒã„ã‚ã ã‘ã©è¦‹ã‚„ã™ã„ã‚¹ã‚¿ã‚¤ãƒ«
                add_excel_table(ws, style_name="TableStyleMedium2")
            elif ws.title == "table_api_matrix":
                add_excel_table(ws, style_name="TableStyleMedium4")
            elif ws.title == "tables_all":
                # ä¸€è¦§ã¯ä¸€ç•ªè–„ã„ã‚¹ã‚¿ã‚¤ãƒ«
                add_excel_table(ws, style_name="TableStyleLight1")

    print(f"âœ… Exported to: {OUTPUT_XLSX}")


if __name__ == "__main__":
    main()
